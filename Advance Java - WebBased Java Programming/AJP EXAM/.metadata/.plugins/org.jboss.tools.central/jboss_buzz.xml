<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Understanding JPA Entity life cycle</title><link rel="alternate" href="http://www.mastertheboss.com/java-ee/jpa/understanding-jpa-entity-life-cycle/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java-ee/jpa/understanding-jpa-entity-life-cycle/</id><updated>2022-12-28T11:24:50Z</updated><content type="html">This article discusses the life cycle of Entity objects in JPA Applications. Understanding the different stages that an Entity goes through is crucial for proper understanding of the JPA framework. The JPA Entity Lifecycle refers to the stages that an entity (a Java object representing a database record) goes through during its existence in a ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to test REST Services with RestAssured</title><link rel="alternate" href="http://www.mastertheboss.com/jboss-frameworks/resteasy/how-to-test-rest-services-with-restassured/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jboss-frameworks/resteasy/how-to-test-rest-services-with-restassured/</id><updated>2022-12-27T16:25:27Z</updated><content type="html">In this tutorial, we will be discussing REST Assured framework to Test JAX-RS Web services. This tutorial is designed for RESTAssured beginners, but even experienced users may find some useful tips and tricks. RestAssured is a Java library that provides a domain-specific language (DSL) for writing powerful, maintainable tests for RESTful APIs. It allows you ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Debugging binaries invoked from scripts with GDB</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/12/27/debugging-binaries-invoked-scripts-gdb" /><author><name>Kevin Buettner</name></author><id>5897d39f-6d3f-4c99-96bf-9d60b21136f9</id><updated>2022-12-27T07:00:00Z</updated><published>2022-12-27T07:00:00Z</published><summary type="html">&lt;p&gt;Suppose you want to use GDB, the GNU debugger for &lt;a href="https://developers.redhat.com/topics/c"&gt;C and C++ programs&lt;/a&gt;, to debug a program invoked from a shell script. You might have trouble knowing what is going on in the program because the script might give it a complicated run-time context, setting environment variables in various ways depending upon the machine, architecture, installed programs, etc. with which it's being run.&lt;/p&gt; &lt;p&gt;A good example of such a script is &lt;code&gt;/usr/bin/firefox&lt;/code&gt;. On my Fedora 35 machine, the &lt;code&gt;firefox&lt;/code&gt; script is 290 lines long. It mostly sets a lot of environment variables, but it also contains commands to make directories, remove files and directories, and make symbolic links. All these changes can have impacts on the binary when it runs. Near the end of the script, a command invokes (via &lt;code&gt;exec&lt;/code&gt;) another script named &lt;code&gt;run-mozilla.sh&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;run-mozilla.sh&lt;/code&gt; script itself is 356 lines long. It also sets environment variables and eventually invokes (also via &lt;code&gt;exec&lt;/code&gt;) the Firefox binary. Additionally, the script provides options that allow you to debug the Firefox binary with a debugger, though for this article we won't use those options.&lt;/p&gt; &lt;p&gt;Use of a wrapper script to set environment variables and then invoke a binary is fairly common. On my Fedora 35 machine, more than 13 percent of the files in &lt;code&gt;/usr/bin&lt;/code&gt; start with either &lt;code&gt;#!/usr/bin/sh&lt;/code&gt; or &lt;code&gt;#!/usr/bin/bash&lt;/code&gt;. An initial &lt;code&gt;#!&lt;/code&gt; string on the first line of a text file is a convention known in Unix and Linux as a &lt;em&gt;shebang.&lt;/em&gt; The line specifies the program that should run the script. In short, lots of programs in the directory named &lt;code&gt;bin&lt;/code&gt; are not binaries. Some culminate in an &lt;code&gt;exec&lt;/code&gt; command to run some other executable, as the &lt;code&gt;run-mozilla.sh&lt;/code&gt; script does.&lt;/p&gt; &lt;p&gt;In the distant past, when attempting to debug programs associated with similar scripts, I'd examine the script and then set up what I perceived to be the relevant environment variables in an interactive shell session. After doing this, I'd invoke GDB in the usual way on the binary. However, it might take a fair amount of time to understand the wrapper script well enough to create an environment comparable to that created when running the script, and the whole procedure is error-prone.&lt;/p&gt; &lt;p&gt;It turns out that there's a far better and easier way to use GDB to debug binaries invoked via a wrapper script.&lt;/p&gt; &lt;h2&gt;Debugging a binary run from a wrapper script via exec&lt;/h2&gt; &lt;p&gt;It's common for wrapper scripts to use the shell's &lt;code&gt;exec&lt;/code&gt; command to run a binary. The &lt;code&gt;exec&lt;/code&gt; command causes the process in which the shell is running to be replaced by that of the binary. This is different from a &lt;em&gt;fork and exec&lt;/em&gt; (which is used to run other non-builtin commands not prefixed by &lt;code&gt;exec&lt;/code&gt;). A fork and exec creates a new process and enables the shell script to continue after the command it invokes has exited.&lt;/p&gt; &lt;p&gt;In order to use GDB to debug a binary invoked by the &lt;code&gt;exec&lt;/code&gt; command, follow these steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Make sure that the script in question uses &lt;code&gt;exec&lt;/code&gt; to invoke the program you are debugging. You can identify whether the wrapper uses &lt;code&gt;exec&lt;/code&gt; by simply searching for &lt;code&gt;exec&lt;/code&gt; in the script. Once you find that command, verify that the &lt;code&gt;exec&lt;/code&gt; command invokes the binary you want to debug. For instance, the last line of the &lt;code&gt;/usr/bin/firefox&lt;/code&gt; script looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;exec $MOZ_LAUNCHER $script_args $MOZ_PROGRAM "$@"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Furthermore, &lt;code&gt;/usr/lib64/firefox/run-mozilla.sh&lt;/code&gt; contains the following line in the shell function &lt;code&gt;moz_run_program&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;exec "$prog" ${1+"$@"}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So the inner and outer wrapper scripts each use an &lt;code&gt;exec&lt;/code&gt; command to run the next script or binary.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Find the name of the shell used by the outermost wrapper script, usually specified by the shebang mentioned earlier. Thus, for Firefox, view the first line by entering:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ head -n 1 /usr/bin/firefox #!/usr/bin/bash&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output shows that the &lt;code&gt;firefox&lt;/code&gt; wrapper script uses the &lt;code&gt;/usr/bin/bash&lt;/code&gt; shell to run the script.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Start GDB by debugging the shell rather than the binary that you want to (eventually) debug. For Firefox, the command could look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gdb -q --args /usr/bin/bash /usr/bin/firefox&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;-q&lt;/code&gt; option just suppresses the copyright notice and other information that's normally printed by GDB when starting up. The &lt;code&gt;--args&lt;/code&gt; option specifies &lt;code&gt;/usr/bin/bash&lt;/code&gt; as the executable file to debug, and &lt;code&gt;/usr/bin/firefox&lt;/code&gt; as a command-line argument to pass once GDB starts the executable.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Once in GDB, use GDB's &lt;code&gt;catch exec&lt;/code&gt; command to cause GDB to stop on an &lt;code&gt;exec&lt;/code&gt; system call:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;(gdb) catch exec&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Use GDB's &lt;code&gt;run&lt;/code&gt; command to start execution:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;(gdb) run&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;When an &lt;code&gt;exec&lt;/code&gt; catchpoint is hit, examine the message to see what binary will be debugged next. If it's just another shell, you probably want to continue. If the next file turns out to be the binary that you're interested in, you can start debugging the binary as usual. Typically, you now place a breakpoint on some function that you know will be hit prior to using &lt;code&gt;continue&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;An example GDB session follows, to demonstrate the steps just described. Note that, toward the end, I place a breakpoint on the &lt;code&gt;main&lt;/code&gt; function and then continue to it.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ &lt;strong&gt;gdb -q --args /usr/bin/bash /usr/bin/firefox&lt;/strong&gt; Reading symbols from /usr/bin/bash... This GDB supports auto-downloading debuginfo from the following URLs: https://debuginfod.fedoraproject.org/ Enable debuginfod for this session? (y or [n]) &lt;strong&gt;y&lt;/strong&gt; Debuginfod has been enabled. To make this setting permanent, add 'set debuginfod enabled on' to .gdbinit. Reading symbols from /home/kev/.cache/debuginfod_client/65289d3e4b67a5f765c63c7ec51c7f28f753ce08/debuginfo... (gdb) &lt;strong&gt;catch exec&lt;/strong&gt; Catchpoint 1 (exec) (gdb) &lt;strong&gt;run&lt;/strong&gt; Starting program: /usr/bin/bash /usr/bin/firefox [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1". &lt;em&gt;[several ‘Detaching after fork from child process' messages snipped]&lt;/em&gt; process 876717 is executing new program: /usr/bin/bash Catchpoint 1 (exec'd /usr/bin/bash), 0x00007ffff7fe7ac0 in _start () from /lib64/ld-linux-x86-64.so.2 (gdb) &lt;strong&gt;continue&lt;/strong&gt; Continuing. [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1". &lt;em&gt;[several ‘Detaching after fork from child process' messages snipped]&lt;/em&gt; process 876717 is executing new program: /usr/lib64/firefox/firefox Catchpoint 1 (exec'd /usr/lib64/firefox/firefox), 0x00007ffff7fe7ac0 in _start () from /lib64/ld-linux-x86-64.so.2 (gdb) &lt;strong&gt;break main&lt;/strong&gt; Breakpoint 2 at 0x55555559a7b0: file /usr/src/debug/firefox-100.0-4.fc35.x86_64/browser/app/nsBrowserApp.cpp, line 259. (gdb) &lt;strong&gt;continue&lt;/strong&gt; Continuing. [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1". [New Thread 0x7ffff77ff640 (LWP 876922)] [Thread 0x7ffff77ff640 (LWP 876922) exited] Thread 1 "firefox" hit Breakpoint 2, main (argc=1, argv=0x7fffffffdb28, envp=0x7fffffffdb38) at /usr/src/debug/firefox-100.0-4.fc35.x86_64/browser/app/nsBrowserApp.cpp:259 259 int main(int argc, char* argv[], char* envp[]) { (gdb)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At this point in the session, GDB has stopped at &lt;code&gt;main&lt;/code&gt; in the Firefox binary. Debugging can proceed normally from this point. You can set additional breakpoints, continue to those breakpoints, examine the stack, look at variables, use the &lt;code&gt;step&lt;/code&gt; or &lt;code&gt;next&lt;/code&gt; commands, etc.&lt;/p&gt; &lt;h2&gt;Debugging programs invoked via fork and exec from a wrapper script&lt;/h2&gt; &lt;p&gt;Things get more complicated when you wish to debug a binary invoked via a &lt;code&gt;fork&lt;/code&gt; and then an &lt;code&gt;exec&lt;/code&gt; from a wrapper script. One complication is that the script might invoke a number of commands, continuing after each one. You have to take care to debug only the binary of interest. Another complication is that, by default, GDB doesn't follow the child after a fork. Fortunately, GDB offers a command to change that default behavior.&lt;/p&gt; &lt;p&gt;A reasonably simple yet interesting script to consider as an example is &lt;code&gt;/usr/bin/zmore&lt;/code&gt;. The &lt;code&gt;zmore&lt;/code&gt; script is part of the gzip package (on Fedora systems). It invokes &lt;code&gt;gzip&lt;/code&gt; (in decompress mode) on the arguments (which are filenames) provided to the script and then pipes the decompressed output to &lt;code&gt;$PAGER&lt;/code&gt; if that environment variable is defined or to &lt;code&gt;more&lt;/code&gt; if it isn't. For this discussion, let's assume that &lt;code&gt;PAGER&lt;/code&gt; is not defined and that we wish to debug &lt;code&gt;more&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The last six lines of &lt;code&gt;/usr/bin/zmore&lt;/code&gt; look like the following. The &lt;code&gt;eval&lt;/code&gt; command simply chooses &lt;code&gt;$PAGER&lt;/code&gt; or &lt;code&gt;more&lt;/code&gt; based on the criteria I just explained:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;for FILE do test $# -lt 2 || printf '::::::::::::::\n%s\n::::::::::::::\n' "$FILE" || break gzip -cdfq -- "$FILE" done 2&gt;&amp;1 | eval ${PAGER-more}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also, the first line of &lt;code&gt;/usr/bin/zmore&lt;/code&gt; shows that the shell used by the script is &lt;code&gt;/usr/bin/sh&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Let's start by making a test file compressed using &lt;code&gt;gzip&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ for i in {1..1000}; do echo $i; done | gzip &gt;testfile.gz&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I won't show the output here, but I suggest running the following commands to make sure that a suitable test file has been created and that &lt;code&gt;more&lt;/code&gt; is being used to output it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ unset PAGER $ zmore testfile.gz&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When you use &lt;code&gt;more&lt;/code&gt; as your pager, the string "--More–" is shown at the bottom of the terminal window.&lt;/p&gt; &lt;p&gt;Now, to debug &lt;code&gt;more&lt;/code&gt; when invoked from &lt;code&gt;zmore&lt;/code&gt;, start with the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ &lt;strong&gt;gdb -q --args /bin/sh zmore testfile.gz&lt;/strong&gt; Reading symbols from /bin/sh... This GDB supports auto-downloading debuginfo from the following URLs: https://debuginfod.fedoraproject.org/ Enable debuginfod for this session? (y or [n]) &lt;strong&gt;y&lt;/strong&gt; Debuginfod has been enabled. To make this setting permanent, add 'set debuginfod enabled on' to .gdbinit. Reading symbols from ~/.cache/debuginfod_client/65289d3e4b67a5f765c63c7ec51c7f28f753ce08/debuginfo… (gdb)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command used to invoke GDB is similar to that shown earlier for debugging Firefox, except that in this case I included the name of the test file as a third &lt;code&gt;--args&lt;/code&gt; argument.&lt;/p&gt; &lt;p&gt;Also, note that I answered &lt;code&gt;y&lt;/code&gt; to the "Enable debuginfod" question. Using debuginfod makes debugging this kind of program easy. Without it, you'd need to manually download debuginfo for each of the programs that you're debugging.&lt;/p&gt; &lt;p&gt;As before, issue the &lt;code&gt;catch exec&lt;/code&gt; command to make GDB stop when the inferior is performing an &lt;code&gt;exec&lt;/code&gt; system call:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;(gdb) &lt;strong&gt;catch exec&lt;/strong&gt; Catchpoint 1 (exec)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now use two commands to change how &lt;code&gt;fork&lt;/code&gt; is handled. The first command, &lt;code&gt;set detach-on-fork off&lt;/code&gt;, helps GDB control both the parent and child processes after a &lt;code&gt;fork&lt;/code&gt;. The second command, &lt;code&gt;set follow-fork-mode child&lt;/code&gt;, causes the child process to be debugged (instead of the parent, which is followed by default). These commands produce no output:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;(gdb) &lt;strong&gt;set detach-on-fork off&lt;/strong&gt; (gdb) &lt;strong&gt;set follow-fork-mode child&lt;/strong&gt; (gdb)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, use the &lt;code&gt;run&lt;/code&gt; command to start the program:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;(gdb) &lt;strong&gt;run&lt;/strong&gt; Starting program: /usr/bin/sh zmore testfile.gz [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1". [Attaching after Thread 0x7ffff7d65740 (LWP 943122) fork to child process 943125] [New inferior 2 (process 943125)] [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1". [Attaching after Thread 0x7ffff7d65740 (LWP 943125) fork to child process 943126] [New inferior 3 (process 943126)] [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1". process 943126 is executing new program: /usr/bin/gzip Reading symbols from /lib64/ld-linux-x86-64.so.2... [Switching to process 943126] Reading symbols from /lib64/ld-linux-x86-64.so.2... Thread 3.1 "gzip" hit Catchpoint 1 (exec'd /usr/bin/gzip), 0x00007ffff7fe7ac0 in _start () from /lib64/ld-linux-x86-64.so.2 (gdb)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Execution has stopped at the &lt;code&gt;exec&lt;/code&gt; catchpoint for &lt;code&gt;/usr/bin/gzip&lt;/code&gt;. If we wanted to debug &lt;code&gt;gzip&lt;/code&gt;, we could place breakpoints and continue, but our goal is instead to debug &lt;code&gt;more&lt;/code&gt;. Look at the inferiors that GDB knows about by entering &lt;code&gt;info inferiors&lt;/code&gt;. Also, since you don't want to debug &lt;code&gt;gzip&lt;/code&gt;, detach from that inferior.&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;(gdb) &lt;strong&gt;info inferiors&lt;/strong&gt; Num Description Connection Executable 1 process 943122 1 (native) /usr/bin/sh 2 process 943125 1 (native) /usr/bin/sh * 3 process 943126 1 (native) /usr/bin/gzip (gdb) &lt;strong&gt;detach&lt;/strong&gt; Detaching from program: /usr/bin/gzip, process 943126 [Inferior 3 (process 943126) detached] (gdb)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;info inferiors&lt;/code&gt; command showed three inferiors: &lt;code&gt;gzip&lt;/code&gt; plus two &lt;code&gt;/bin/sh&lt;/code&gt; inferiors. I think it's likely that inferior #2 is for the &lt;code&gt;for&lt;/code&gt; loop being piped to &lt;code&gt;more&lt;/code&gt;, so it's likely that you want to switch to inferior #1. But let's not assume that and instead switch to inferior #2, continue, and see what happens:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;(gdb) &lt;strong&gt;inferior 2&lt;/strong&gt; [Switching to inferior 2 [process 943125] (/usr/bin/sh)] [Switching to thread 2.1 (Thread 0x7ffff7d65740 (LWP 943125))] #0 arch_fork (ctid=Reading symbols from /lib64/ld-linux-x86-64.so.2... 0x7ffff7d65a10) at ../sysdeps/unix/sysv/linux/arch-fork.h:52 52 ret = INLINE_SYSCALL_CALL (clone, flags, 0, NULL, ctid, 0); (gdb) &lt;strong&gt;continue&lt;/strong&gt; Continuing. [Inferior 2 (process 943125) exited normally]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That inner shell process won't always exit at this point. If it doesn't exit and appears to hang (while it's either reading input or waiting for &lt;code&gt;more&lt;/code&gt; to consume its output), use Ctrl-C to interrupt GDB and then use the &lt;code&gt;detach&lt;/code&gt; command on inferior #2. After switching to inferior #1 (as shown soon), if you see that it is interrupted due to SIGINT, just continue again. These extra steps can be avoided by simply switching to the desired inferior in the first place.&lt;/p&gt; &lt;p&gt;Now let's switch to inferior #1 and continue:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;(gdb) &lt;strong&gt;inferior 1&lt;/strong&gt; [Switching to inferior 1 [process 943122] (/usr/bin/sh)] [Switching to thread 1.1 (Thread 0x7ffff7d65740 (LWP 943122))] #0 arch_fork (ctid=0x7ffff7d65a10) at ../sysdeps/unix/sysv/linux/arch-fork.h:52 52 ret = INLINE_SYSCALL_CALL (clone, flags, 0, NULL, ctid, 0); (gdb) &lt;strong&gt;continue&lt;/strong&gt; Continuing. [Attaching after Thread 0x7ffff7d65740 (LWP 943122) fork to child process 949916] [New inferior 4 (process 949916)] [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1". [Attaching after Thread 0x7ffff7d65740 (LWP 949916) fork to child process 949917] [New inferior 5 (process 949917)] [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1". process 949917 is executing new program: /usr/bin/more Reading symbols from /lib64/ld-linux-x86-64.so.2... [Switching to process 949917] Reading symbols from /lib64/ld-linux-x86-64.so.2... Thread 5.1 "more" hit Catchpoint 1 (exec'd /usr/bin/more), 0x00007ffff7fe7ac0 in _start () from /lib64/ld-linux-x86-64.so.2 (gdb)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At this point, we've hit the &lt;code&gt;exec&lt;/code&gt; catchpoint for &lt;code&gt;/usr/bin/more&lt;/code&gt;, which is what we wanted to debug. Let's put a breakpoint on &lt;code&gt;main&lt;/code&gt; and continue to it:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;(gdb) &lt;strong&gt;break main&lt;/strong&gt; Reading symbols from /lib64/ld-linux-x86-64.so.2... Breakpoint 2 at 0x555555556c40: main. (3 locations) (gdb) &lt;strong&gt;continue&lt;/strong&gt; Continuing. [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1". Thread 5.1 "more" hit Breakpoint 2, main (argc=1, argv=0x7fffffffddf8) at text-utils/more.c:2009 2009 { (gdb)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that we're stopped at the &lt;code&gt;main&lt;/code&gt; function of &lt;code&gt;/usr/bin/more&lt;/code&gt;, let's restore GDB's fork-related settings to their defaults. This is recommended because, should &lt;code&gt;more&lt;/code&gt; fork and exec some other program, we want to stay within &lt;code&gt;more&lt;/code&gt; instead of following the child, whatever it is:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;(gdb) set detach-on-fork on (gdb) set follow-fork-mode parent (gdb)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After restoring these settings, you can proceed to debug &lt;code&gt;/usr/bin/more&lt;/code&gt; as normal using less esoteric GDB commands.&lt;/p&gt; &lt;h2&gt;Using gdbserver to avoid writing to the same terminal as GDB&lt;/h2&gt; &lt;p&gt;If you debug a program that produces output, like &lt;code&gt;more&lt;/code&gt; in the previous section, output from the debugged program is normally sent to the same terminal as that used by GDB. If you attempt to simply enter &lt;code&gt;continue&lt;/code&gt;, you'll find that GDB will stop due to a SIGTTOU signal. Further attempts to continue will repeatedly stop due to the SIGTTOU signal.&lt;/p&gt; &lt;p&gt;This problem can be avoided by running &lt;code&gt;zmore&lt;/code&gt; with &lt;code&gt;gdbserver&lt;/code&gt; in one terminal and connecting to the program from GDB running in another terminal. The command used to run &lt;code&gt;gdbserver&lt;/code&gt; looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gdbserver localhost:12345 zmore testfile.gz Process zmore created; pid = 1550989 Listening on port 12345&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(If port 12345 is already in use, simply pick another port. Make sure you use the same port number when connecting to &lt;code&gt;gdbserver&lt;/code&gt; from GDB.)&lt;/p&gt; &lt;p&gt;Connect to &lt;code&gt;gdbserver&lt;/code&gt; from GDB as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gdb -q (gdb) target remote localhost:12345 Remote debugging using localhost:12345 Reading /usr/bin/bash from remote target... &lt;em&gt;[lots of output snipped]&lt;/em&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once connected, the session proceeds as shown earlier except that you need to use the &lt;code&gt;continue&lt;/code&gt; command in place of the &lt;code&gt;run&lt;/code&gt; command.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has shown how to use GDB to debug binaries run from a shell script. The main ideas presented were:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Use GDB to debug the shell binary used for running the script. The name of the script plus arguments to the script become arguments to the shell command.&lt;/li&gt; &lt;li&gt;Issue GDB's &lt;code&gt;catch exec&lt;/code&gt; command to cause GDB to stop when an &lt;code&gt;exec&lt;/code&gt; system call is encountered during program execution.&lt;/li&gt; &lt;li&gt;When debugging binaries invoked via a fork and exec, two additional commands, &lt;code&gt;set detach-on-fork off&lt;/code&gt; and &lt;code&gt;set follow-fork mode child&lt;/code&gt;, change GDB's default behavior with regard to forks.&lt;/li&gt; &lt;li&gt;When an &lt;code&gt;exec&lt;/code&gt; catchpoint is reached, start debugging the binary as normal if it's the one that you wish to debug. If not, other commands are available to continue either the current inferior or some other inferior until a suitable &lt;code&gt;exec&lt;/code&gt; catchpoint is reached.&lt;/li&gt; &lt;li&gt;&lt;code&gt;gdbserver&lt;/code&gt; can be used in situations where it's confusing to distinguish output from GDB and output from the program, or where normal debugging is simply not possible due to continued receipt of SIGTTOU.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/12/27/debugging-binaries-invoked-scripts-gdb" title="Debugging binaries invoked from scripts with GDB"&gt;Debugging binaries invoked from scripts with GDB&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Kevin Buettner</dc:creator><dc:date>2022-12-27T07:00:00Z</dc:date></entry><entry><title>Manage OpenShift Streams for Apache Kafka with AKHQ</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/12/26/manage-openshift-streams-apache-kafka-akhq" /><author><name>Duncan Doyle</name></author><id>f3e9a651-4395-4243-a975-06fa08224334</id><updated>2022-12-26T07:00:00Z</updated><published>2022-12-26T07:00:00Z</published><summary type="html">&lt;p&gt;At Red Hat, we are often asked what consoles and graphical user interfaces (GUIs) can be used with our Kafka products, &lt;a href="https://www.redhat.com/en/resources/amq-streams-datasheet"&gt;Red Hat AMQ Streams&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;. Because our products are fully based on the upstream &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka project&lt;/a&gt;, most available consoles and GUIs designed to work with Kafka also work with our Kafka products. This article illustrates the ease of integration through a look at &lt;a data-href="https://akhq.io" href="https://akhq.io/"&gt;AKHQ&lt;/a&gt;, an open source GUI for Apache Kafka.&lt;/p&gt; &lt;h2&gt;Using Apache Kafka&lt;/h2&gt; &lt;p&gt;Apache Kafka has quickly become the leading event-streaming platform, enabling organizations to unlock and use their data in new and innovative ways. With Apache Kafka, companies can bring more value to their products by processing real-time events more quickly and accurately.&lt;/p&gt; &lt;p&gt;At a high level, Apache Kafka's architecture is quite simple. It's based on a few concepts such as brokers, topics, partitions, producers, and consumers. However—as with any system—when you deploy, operate, manage, and monitor a production Kafka cluster, things can quickly become complex. To use and manage Kafka clusters in both development and production environments, there are numerous tools on the market, both commercial and open source. These tools range from scripts, GUIs, and powerful command-line interfaces (CLIs) to full monitoring, management, and governance platforms. Each type of tool offers value in specific parts of the software development cycle.&lt;/p&gt; &lt;p&gt;This article shows how to connect AKHQ to a Kafka instance in Red Hat OpenShift Streams for Apache Kafka, a managed cloud service. Using our no-cost, 48-hour trial of OpenShift Streams, you can follow along with the steps. By the end of the article, you will be able to use AKHQ to manage your Kafka instance.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To follow the instructions in this article, you'll need the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;A Kafka instance in Red Hat OpenShift Streams for Apache Kafka (either a trial instance or full instance). A 48-hour trial instance is available at no cost. Go to the &lt;a data-href="https://console.redhat.com/application-services/streams" href="https://console.redhat.com/application-services/streams"&gt;Red Hat console&lt;/a&gt;, log in with your Red Hat account (or create one), and create a trial instance. You can learn how to create your first Kafka instance in &lt;a data-href="https://console.redhat.com/application-services/learning-resources?quickstart=getting-started" href="https://console.redhat.com/application-services/learning-resources?quickstart=getting-started"&gt;this quick start guide&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The &lt;code&gt;rhoas&lt;/code&gt; CLI. This is a powerful command-line interface for managing various Red Hat OpenShift Application Services resources, such as Kafka instances, Service Registry instances, service accounts, and more. Download the CLI at this &lt;a href="https://github.com/redhat-developer/app-services-cli"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A container runtime such as Podman or Docker to run the AKHQ container image used in this article.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The sections that follow show how to generate connection information for a Kafka instance in OpenShift Streams and then use this information to connect AKHQ to the Kafka instance.&lt;/p&gt; &lt;h2 id="_creating_a_service_account"&gt;Creating a service account&lt;/h2&gt; &lt;p&gt;For AKHQ to connect to your Kafka instance, it needs credentials to authenticate with OpenShift Streams for Apache Kafka. In the OpenShift Application Services world, this means that you must create a service account. You can do this using the web console, the CLI, or a REST API call. The following steps show how to do so using the web console:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Navigate to &lt;a data-href="https://console.redhat.com/application-services/service-accounts" href="https://console.redhat.com/application-services/service-accounts"&gt;https://console.redhat.com/application-services/service-accounts&lt;/a&gt; and click &lt;strong&gt;Create service account&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Enter a name for the service account. In this case, let's call it &lt;code&gt;akhq-sa&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Copy the generated &lt;strong&gt;Client ID&lt;/strong&gt; and &lt;strong&gt;Client secret&lt;/strong&gt; values to a secure location. You'll specify these credentials when configuring a connection to the Kafka instance.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;After you save the generated credentials to a secure location, select the confirmation check box and click &lt;strong&gt;Close&lt;/strong&gt;, as shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/service-account.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/service-account.png?itok=2D_nlEro" width="600" height="286" alt="A dialog box shows that security credentials were generated." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A dialog box shows that security credentials were generated. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Configuring Kafka ACLs&lt;/h2&gt; &lt;p&gt;Before AKHQ can use the service account to connect to your Kafka instance in OpenShift Streams, you need to configure authorization of the account using Kafka &lt;em&gt;Access Control List&lt;/em&gt; (ACL) permissions. At a minimum, you need to give the service account permissions to create and delete topics and produce and consume messages.&lt;/p&gt; &lt;p&gt;To configure ACL permissions for the service account, this article uses the &lt;code&gt;rhoas&lt;/code&gt; CLI. You can also use the web console or the REST API to configure the permissions.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: In this article, we show how to use AKHQ to manage topics and consumer groups. Therefore, we set only the ACLs required to manage those resources. If you want to manage other Kafka resources—for example, the Kafka broker configuration—you might need to configure additional ACL permissions for your service account. However, always be sure to set &lt;em&gt;only&lt;/em&gt; the permissions needed for your use case. Having minimal permissions limits the attack surface of your Kafka cluster, improving security.&lt;/p&gt; &lt;p&gt;The following steps show how to set the required ACL permissions for your service account.&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Log in to the &lt;code&gt;rhoas&lt;/code&gt; CLI:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;rhoas login&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A login flow opens in your web browser.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Log in with the Red Hat account that you used to create your OpenShift Streams instance.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Specify the Kafka instance in OpenShift Streams that you would like to use:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;rhoas kafka use&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;rhoas kafka use&lt;/code&gt; command sets the OpenShift Streams instance in the context of your CLI, meaning that subsequent CLI operations (for example, setting ACLs), are performed against this Kafka instance.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Set the required Kafka ACL permissions, as shown in the following example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;export CLIENT_ID=&lt;your-service-account-client-id&gt; rhoas kafka acl grant-access --producer --consumer --service-account $CLIENT_ID --topic "*" --group "*" -y &amp;&amp; \ rhoas kafka acl create --operation delete --permission allow --topic "*" --service-account $CLIENT_ID -y &amp;&amp; \ rhoas kafka acl create --operation alter-configs --permission allow --topic "*" --service-account $CLIENT_ID -y&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The example invokes the &lt;code&gt;grant-access&lt;/code&gt; subcommand to set ACL permissions to produce and consume messages in any topic in the Kafka instance. To fully manage topics from AKHQ, subsequent commands allow the &lt;code&gt;delete&lt;/code&gt; and &lt;code&gt;alter-configs&lt;/code&gt; operations on any topic. Output from the commands is shown in Figure 2, indicating that the desired permissions and ACLs were created.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rhoas-set-acls.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rhoas-set-acls.png?itok=dL0CrCHs" width="600" height="309" alt="Output from the ACL commands show the permissions and ACLs generated." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Output from the ACL commands show the permissions and ACLs generated. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Output from the ACL commands shows the permissions and ACLs generated.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ol&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This example uses the * wildcard character to set permissions for all topics in the Kafka cluster. You can limit access by setting the permissions for a specific topic name or for a set of topics using a prefix.&lt;/p&gt; &lt;h2 id="_connecting_akhq_to_openshift_streams_for_apache_kafka"&gt;Connecting AKHQ to OpenShift Streams for Apache Kafka&lt;/h2&gt; &lt;p&gt;With your ACL permissions in place, you can now configure AKHQ to connect to your Kafka instance in OpenShift Streams. There are multiple ways to run AKHQ. The &lt;a data-href="https://akhq.io/docs/installation.html" href="https://akhq.io/docs/installation.html"&gt;AKQH installation documentation&lt;/a&gt; describes the various options in detail.&lt;/p&gt; &lt;p&gt;This article runs AKHQ in a container. This means that you need a container runtime such as Podman or Docker. We show how to run the container using Docker Compose, but you can also use Podman Compose with the same compose file.&lt;/p&gt; &lt;p&gt;The AKHQ configuration in this example is a very basic configuration that connects to an OpenShift Streams Kafka instance using Simple Authentication Security Layer (SASL) OAuthBearer authentication. The configuration uses the client ID and client secret values of your service account and an OAuth token endpoint URL, which is required for authentication with &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat's single sign-on (SSO) technology&lt;/a&gt;. The example configuration looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;akhq: # list of kafka cluster available for akhq connections: openshift-streams-kafka: properties: bootstrap.servers: "${BOOTSTRAP_SERVER}" security.protocol: SASL_SSL sasl.mechanism: OAUTHBEARER sasl.jaas.config: &gt; org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required oauth.client.id="${CLIENT_ID}" oauth.client.secret="${CLIENT_SECRET}" oauth.token.endpoint.uri="${OAUTH_TOKEN_ENDPOINT_URI}" ; sasl.login.callback.handler.class: io.strimzi.kafka.oauth.client.JaasClientOauthLoginCallbackHandler&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You'll notice that the configuration also uses the &lt;code&gt;OauthLoginCallbackHandler&lt;/code&gt; class from the &lt;a data-href="https://strimzi.io/" href="https://strimzi.io/"&gt;Strimzi project&lt;/a&gt;. This callback handler class is packaged by default with AKHQ, enabling you to use OAuthBearer authentication against OpenShift Streams.&lt;/p&gt; &lt;p&gt;The configuration shown is included in the &lt;code&gt;docker-compose.yml&lt;/code&gt; file that we'll use to run our containerized AKHQ instance. You can find the &lt;code&gt;docker-compose.yml&lt;/code&gt; file in &lt;a data-href="https://github.com/DuncanDoyle/rhosak-akhq-blog" href="https://github.com/DuncanDoyle/rhosak-akhq-blog"&gt;this GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The following steps show to use Docker Compose to connect AKHQ to your Kafka instance in OpenShift Streams:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Clone the GitHub repository that has the example &lt;code&gt;docker-compose.yml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/DuncanDoyle/rhosak-akhq-blog.git cd rhosak-akhq-blog&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Set environment variables that define connection information for your Kafka instance.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;export CLIENT_ID=&lt;your-service-account-client-id&gt; export CLIENT_SECRET=&lt;your-service-account-client-secret&gt; export BOOTSTRAP_SERVER=&lt;your-kafka-bootstrap-server-url-and-port&gt; export OAUTH_TOKEN_ENDPOINT_URI=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These environment variables specify the client ID and client secret values of your service account, the OAuth token endpoint URL used for authentication with Red Hat's SSO service, and the bootstrap server URL and port of your Kafka instance.&lt;/p&gt; &lt;p&gt;You can get the bootstrap server information for your Kafka instance in the OpenShift Streams web console or by using the &lt;code&gt;rhoas kafka describe&lt;/code&gt; CLI command. You can also get the OAuth token endpoint URL in the web console. However, because this URL is a static value in OpenShift Streams, you can simply set it to &lt;code&gt;&lt;a data-href="https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token" href="https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token"&gt;https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token&lt;/a&gt;&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Figure 3 shows how to get the bootstrap server and token URL in the web console.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rhosak-bootstrap-server.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rhosak-bootstrap-server.png?itok=okypM3oh" width="600" height="286" alt="Choose your Kafka instance to find the bootstrap server and token URL." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Choose your Kafka instance to find the bootstrap server and token URL. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Start AKHQ using &lt;code&gt;docker-compose&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;docker-compose up&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The AKHQ management console becomes available at &lt;code&gt;http://localhost:8080&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;If you've configured everything correctly, when you hover your mouse over the datastore icon, you should see an &lt;code&gt;openshift-streams-kafka&lt;/code&gt; connection, as shown in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rhosak-akhq-empty.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rhosak-akhq-empty.png?itok=FU1QfFez" width="600" height="314" alt="A Kafka connection is available, with no topics." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: A Kafka connection is available, with no topics. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2 id="_managing_openshift_streams"&gt;Managing OpenShift Streams&lt;/h2&gt; &lt;p&gt;With the AKHQ management console connected, you can now use the console to interact with your Kafka instance in OpenShift Streams.&lt;/p&gt; &lt;p&gt;The following steps show how to create a topic, produce some data, and inspect the data that you've produced to the topic. You can use the OpenShift Streams web console and &lt;code&gt;rhoas&lt;/code&gt; CLI to do these things, but remember that the point of this example is to show how you can use the AKHQ console!&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Create a topic. In the lower-right corner of the AKHQ console, click &lt;strong&gt;Create a topic&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;On the topic creation page, name the topic &lt;code&gt;my-topic&lt;/code&gt;, keep the default values for all the other options, and click &lt;strong&gt;Create&lt;/strong&gt;, as shown in Figure 5.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/akhq-create-topic.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/akhq-create-topic.png?itok=vRoCjxoN" width="600" height="315" alt="The "Create a topic" page lets you assign a name and other attributes." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: The "Create a topic" page lets you assign a name and other attributes. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;If you have set ACL permissions on the service account correctly, you see the topic that you just created. You can also see the same topic in the OpenShift Streams web console. Figure 6 shows the same topic as it appears in both consoles.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/akhq-topic-created.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/akhq-topic-created.png?itok=8123dqOo" width="600" height="285" alt="Topic in the AKHQ console and the OpenShift Streams console." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Topic in the AKHQ console and the OpenShift Streams console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;To perform tasks such as inspecting and producing messages, viewing consumer groups, inspecting consumer lag, and so on, click your new topic in the AKHQ console. Figure 7 shows a message in JSON format that has come into the topic, and Figure 8 shows statistics about the topic. &lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/akhq-messages.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/akhq-messages.png?itok=DQehnZMn" width="600" height="314" alt="A message appears in JSON format in the topic." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: A message appears in JSON format in the topic. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig8_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig8_1.png?itok=_YUwWVDL" width="600" height="313" alt="Screenshot showing statistics about your Kafka topic." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: Statistics about your Kafka topic. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2 id="_conclusion"&gt;Red Hat simplifies access to open source tools&lt;/h2&gt; &lt;p&gt;This article has shown how to manage and monitor a Kafka instance in Red Hat OpenShift Streams for Apache Kafka using AKHQ. For more information about AKHQ, see the &lt;a data-href="https://akhq.io/docs/" href="https://akhq.io/docs/"&gt;AKHQ documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;On a larger scale, the article illustrates the ability to use popular tools from the open source Kafka ecosystem with Red Hat's managed cloud services. This openness gives you the flexibility you need when building enterprise-scale systems based on open source technologies. The use of open standards and non-proprietary APIs and protocols in Red Hat service offerings enables seamless integration with various technologies.&lt;/p&gt; &lt;p&gt;If you haven't yet done so, please visit the &lt;a data-href="https://console.redhat.com/" href="https://console.redhat.com/"&gt;Red Hat Hybrid Cloud Console&lt;/a&gt; for more information about OpenShift Streams for Apache Kafka, as well as our other service offerings. OpenShift Streams for Apache Kafka provides a 48-hour trial version of our product at no cost. We encourage you to give it a spin.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/12/26/manage-openshift-streams-apache-kafka-akhq" title="Manage OpenShift Streams for Apache Kafka with AKHQ"&gt;Manage OpenShift Streams for Apache Kafka with AKHQ&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Duncan Doyle</dc:creator><dc:date>2022-12-26T07:00:00Z</dc:date></entry><entry><title type="html">Announcing Dashbuilder Quarkus Extension</title><link rel="alternate" href="https://blog.kie.org/2022/12/announcing-dashbuilder-quarkus-extension.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2022/12/announcing-dashbuilder-quarkus-extension.html</id><updated>2022-12-23T16:53:23Z</updated><content type="html">We are glad to announce that we released the ! You can now render dashboards directly in your Quarkus app! INSTALLATION To start using this extension simply add the following dependency to your quarkus app pom.xml: &lt;dependency&gt; &lt;groupId&gt;io.quarkiverse.dashbuilder&lt;/groupId&gt; &lt;artifactId&gt;quarkus-dashbuilder&lt;/artifactId&gt; &lt;version&gt;0.26.1&lt;/version&gt; &lt;/dependency&gt; Then your *.dash.(yaml|yml|json) files will be available in /dashboards web context. CONFIGURATION You can map specific dashboard files using the system property quarkus.dashbuilder.dashboards and change the dashboards web context using quarkus.dashbuilder.path: EXAMPLES We also have three examples ready for use: * Hello World: This is the simplest example where the dashboards run on the client and use an inline dataset. Remember that datasets can be any JSON Array declared directly in the dataset or coming from a URL. Learn more about it in the article . * Metrics: Datasets can also have a pre-processing for some specific formats. This is the case for . In this example we consume Quarkus Micrometer metrics.  * DataSet: It is also possible to consume data generated from the Quarkus application. In this example we build a real time chart from data coming from a Quarkus endpoint. LEARN MORE ABOUT DASHBUILDER DEVELOPMENT is a tool for building dashboards and data visualizations. It runs entirely on client and you can develop dashboards using YAML on the or using the .  Links to learn more: * * * s CONCLUSION In this article we announced the Dashbuilder Quarkus extension. We have plans for a better editor developer experience and new Dashbuilder features, so stay tuned! The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>Configuring Kubernetes operands through custom resources</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/12/23/configuring-kubernetes-operands-through-custom-resources" /><author><name>Peter Braun</name></author><id>c7d51447-a83d-46fb-8afa-2e9566d0f392</id><updated>2022-12-23T07:00:00Z</updated><published>2022-12-23T07:00:00Z</published><summary type="html">&lt;p&gt;Operators in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; often allow application developers to configure low-level aspects of their operands and secondary resources. Typically, such settings are made available on the custom resource and reconciled into the operand.&lt;/p&gt; &lt;p&gt;An example of this is the &lt;code&gt;Grafana&lt;/code&gt; custom resource of the &lt;a href="https://github.com/grafana-operator/grafana-operator"&gt;Grafana Operator&lt;/a&gt;. It exposes many configuration options that are reflected in the Grafana configuration file, but also allows you to configure properties of the Kubernetes resources in your Grafana installation. For example, you can add additional ports to your service, mount secrets into the Grafana pod, or expose additional environment variables.&lt;/p&gt; &lt;p&gt;These fields in the custom resource are reconciled and applied to the respective Kubernetes resource. This article describes some problems with this approach, describes an alternative approach that is currently under development, and weighs the strengths and weaknesses of the two approaches.&lt;/p&gt; &lt;h2&gt;The problem with exposing hand-picked configuration options&lt;/h2&gt; &lt;p&gt;The issue with hand-picking properties of Kubernetes resources, exposing them on a custom resource, and then reconciling them is that it's hard to foresee what a user might want to modify. For the Grafana Operator, we often get requests to make additional fields of underlying resources configurable through the custom resource.&lt;/p&gt; &lt;p&gt;Additionally, Kubernetes resources change over time. Granted, that happens rather slowly, but it does happen—it happened when Ingress was promoted to v1, for instance.&lt;/p&gt; &lt;p&gt;Another issue with this approach is ease of use. Someone who is already familiar with configuring, say, a route, now needs to learn how to do that through your custom resource. And often only a subset of the available options is exposed.&lt;/p&gt; &lt;h2&gt;A better approach&lt;/h2&gt; &lt;p&gt;How could the definition of custom resources be improved for both application developers and the people who release resources such as Grafana? In the upcoming version of the Grafana Operator, we have started exposing raw Kubernetes resources in the custom resource. To configure a deployment, for instance, you will have access to a &lt;code&gt;Deployment&lt;/code&gt; object complete with the official &lt;code&gt;spec&lt;/code&gt; and &lt;code&gt;metadata&lt;/code&gt;. The same goes for all other resources that are managed by the Operator, the &lt;code&gt;ServiceAccount&lt;/code&gt;, the &lt;code&gt;Route&lt;/code&gt; or &lt;code&gt;Ingress&lt;/code&gt;, and the &lt;code&gt;Service&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;You don't have to learn how to configure a resource through the Grafana Operator. Instead, you can focus on the Kubernetes resources you want to configure and do the configuration in the usual manner.&lt;/p&gt; &lt;h2&gt;Difficulties&lt;/h2&gt; &lt;p&gt;Sounds easy? Not quite. There are a few obstacles to overcome.&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Some Kubernetes resource definitions, such as &lt;code&gt;Deployment&lt;/code&gt;, are huge and will bloat your custom resource definition (CRD).&lt;/li&gt; &lt;li&gt;Our new way of exposing resources is not suitable for partial specification, which is what application developers usually want most.&lt;/li&gt; &lt;li&gt;We don't yet have a merge strategy to combine Operator defaults and custom overrides.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;We can tackle those issues. The following subsections cover each issue.&lt;/p&gt; &lt;h3&gt;CRD bloat&lt;/h3&gt; &lt;p&gt;To address CRD bloat, we're going to strip the descriptions. When using kubebuilder to generate the CRDs from the code, we can pass the following parameter, which cuts down the size of our CRDs by two thirds:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;crd:maxDescLen=0&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Partial specification&lt;/h3&gt; &lt;p&gt;Let me specify what problem we're trying to solve here. Resources such as &lt;code&gt;Deployment&lt;/code&gt; come with optional and mandatory fields. That is also true for the deployment &lt;code&gt;spec&lt;/code&gt; in our CRD. As soon as an application developer adds a non-empty &lt;code&gt;spec&lt;/code&gt; to a &lt;code&gt;Deployment&lt;/code&gt;, they are required to &lt;code&gt;spec&lt;/code&gt; out all the mandatory fields as well. This is not ideal. You might be interested in just overriding the replicas, without providing a full pod template.&lt;/p&gt; &lt;p&gt;The solution to this problem is not as simple as adding another parameter. We came across the solution while looking at Banzaicloud's &lt;a href="https://github.com/banzaicloud/operator-tools"&gt;operator-tools&lt;/a&gt; project. The idea is to provide your own definition of the &lt;code&gt;spec&lt;/code&gt; that has all the same fields but no mandatory fields.&lt;/p&gt; &lt;p&gt;For example, the original deployment &lt;code&gt;spec&lt;/code&gt; defines the pod template like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;type DeploymentSpec struct { ... Template v1.PodTemplateSpec `json:"template"` ... }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In our own definition, we define &lt;code&gt;v1.PodTemplateSpec&lt;/code&gt; as a pointer and add the &lt;code&gt;omitempty&lt;/code&gt; tag to prevent the serializer from adding an empty key:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;type CustomDeploymentSpec struct { ... Template *v1.PodTemplateSpec `json:"template,omitempty"` ... }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We also define our own &lt;code&gt;Deployment&lt;/code&gt; type:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;type CustomDeployment struct { ObjectMeta ObjectMeta `json:"metadata,omitempty"` Spec CustomDeploymentSpec `json:"spec,omitempty"` }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This gives us a resource with the same structure as a &lt;code&gt;Deployment&lt;/code&gt;, but all the top-level fields are optional.&lt;/p&gt; &lt;h3&gt;Merge strategy&lt;/h3&gt; &lt;p&gt;All that's left to do now is create a merge strategy to merge the overridden, custom deployment with the existing one. Our policy prefers to keep the existing fields in the original resource unless they are the defaults, and we ignore empty fields in the overridden resource.&lt;/p&gt; &lt;p&gt;Thankfully, Kubernetes's own &lt;code&gt;apimachinery&lt;/code&gt; library contains everything we need in its &lt;code&gt;strategicpatch&lt;/code&gt; package. &lt;code&gt;apimachinery&lt;/code&gt; deals with schemas and conversion. &lt;code&gt;strategicpatch&lt;/code&gt; compares two JSON representations of objects and produces a patch that can be applied to the resource definition.&lt;/p&gt; &lt;p&gt;Again, the operator-tools package contains an implementation of a merge function using &lt;code&gt;strategicpatch&lt;/code&gt;, and we use a slightly modified version of it in the Grafana Operator.&lt;/p&gt; &lt;h2&gt;Merging a custom resource field&lt;/h2&gt; &lt;p&gt;Let's see this in action. The default Grafana deployment created by the Operator looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: "1" name: grafana-a-deployment namespace: grafana spec: replicas: 1 selector: matchLabels: app: grafana-a strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We want to override the strategy to &lt;code&gt;Recreate&lt;/code&gt;, so we provide the following deployment in the Grafana custom resource &lt;code&gt;spec&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt; spec: deployment: spec: strategy: type: Recreate&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We don't need to provide any of the fields that are mandatory for deployments, only the field we are interested in overriding: &lt;code&gt;spec.strategy.type&lt;/code&gt;. This produces an updated deployment with a strategy set to &lt;code&gt;Recreate&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Evaluating the strategy of exposing resources&lt;/h2&gt; &lt;p&gt;Exposing raw Kubernetes resources is a powerful way for users to configure operands. Existing knowledge and documentation can be applied. The Operator provides everything that can be configured out of the box.&lt;/p&gt; &lt;p&gt;But disadvantages remain:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The size of the CRD still increases considerably. If you need to configure a large number of resources, this process might not be a good choice.&lt;/li&gt; &lt;li&gt;The process comes with a risk of misconfiguration. Application developers can override settings that the Operator or an operand depends on.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;But we believe that, overall, the advantages of such a flexible configuration system usually outweigh the disadvantages.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/12/23/configuring-kubernetes-operands-through-custom-resources" title="Configuring Kubernetes operands through custom resources"&gt;Configuring Kubernetes operands through custom resources&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Peter Braun</dc:creator><dc:date>2022-12-23T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.31.1 released!</title><link rel="alternate" href="https://blog.kie.org/2022/12/kogito-1-31-1-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/12/kogito-1-31-1-released.html</id><updated>2022-12-23T00:52:22Z</updated><content type="html">We are glad to announce that the Kogito 1.31.1 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Quarkus 2.15 integration * Upgraded Drools to version 8.31.1 to align with newly release drools-drl-quarkus-extension Quarkus extension, for more information, visit the following . For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.25.0 artifacts are available at the . A detailed changelog for 1.31.1 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">How to capture a Thread dump in Java</title><link rel="alternate" href="http://www.mastertheboss.com/java/how-to-capture-a-thread-dump-in-java/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/how-to-capture-a-thread-dump-in-java/</id><updated>2022-12-22T08:05:54Z</updated><content type="html">In this tutorial, we will learn how to capture a thread dump in Java using different methods depending on the operating system and the environment in which the Java application is running. A thread dump is a snapshot of the current state of all threads in a Java application. It is useful for diagnosing problems ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Why we added restartable sequences support to glibc in RHEL 9</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/12/22/restartable-sequences-support-glibc-rhel-9" /><author><name>Florian Weimer</name></author><id>bd5af374-cf16-408f-97ab-8741b42eaaaf</id><updated>2022-12-22T07:00:00Z</updated><published>2022-12-22T07:00:00Z</published><summary type="html">&lt;p&gt;Restartable sequences (rseq) are a Linux feature that can maintain per-CPU data structures in userspace without relying on atomic instructions. A restartable sequence is written under the assumption that it runs from beginning to end without the kernel interrupting it and running some other code on that CPU. It can therefore access per-CPU data without further synchronization.&lt;/p&gt; &lt;p&gt;Restartable refers to the fallback mechanism that kicks in if the kernel has to reschedule execution. In this case, control is transferred to a fallback path, which can retry the execution or use a different algorithm to implement the required functionality. It turns out that this facility is sufficient to implement a variety of algorithms using per-CPU data, especially if combined with an explicit memory barrier system call.&lt;/p&gt; &lt;h2&gt;Why add rseq to Red Hat Enterprise Linux?&lt;/h2&gt; &lt;p&gt;There is not much software yet that uses restartable sequences for production purposes. Google's &lt;a href="https://github.com/google/tcmalloc"&gt;tcmalloc&lt;/a&gt; is one example. But the rseq kernel facility provides one more benefit. The kernel writes the number of the currently running CPU to a data structure shared with userspace. This is sufficient for a user-space-only (and therefore fast) implementation of the POSIX &lt;a href="https://man7.org/linux/man-pages/man3/sched_getcpu.3.html"&gt;sched_getcpu function&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Why does sched_getcpu performance matter? Some database software needs a fast sched_getcpu function for optimization purposes. Traditionally, Linux provides the &lt;a href="https://man7.org/linux/man-pages/man2/getcpu.2.html"&gt;getcpu system call&lt;/a&gt; to obtain the number of the current CPU, but the system call overhead, unfortunately, defeats its use for performance optimizations. Some architectures already provide a fast &lt;a href="https://man7.org/linux/man-pages/man2/getcpu.2.html"&gt;getcpu function&lt;/a&gt; (the Linux variant of sched_getcpu) via the &lt;a href="https://man7.org/linux/man-pages/man7/vdso.7.html"&gt;vDSO acceleration mechanism&lt;/a&gt;, but it turns out that this vDSO approach is impossible to implement on AArch64. Other architectures use some hidden and otherwise unused CPU state to stash the CPU number, but there is simply no available CPU state on AArch64 that could be used for this purpose. This means that for performance parity, AArch64 really needs support for restartable sequences.&lt;/p&gt; &lt;p&gt;In Red Hat Enterprise Linux (RHEL), the sched_getcpu function is provided by the  &lt;a href="https://www.gnu.org/s/libc/"&gt;GNU C Library&lt;/a&gt; (glibc). We did not want to build a custom glibc variant just for AArch64, which is why we chose to enable rseq on all architectures. Furthermore, the rseq-based sched_getcpu function turned out to be slightly faster than the previous vDSO-based implementation.&lt;/p&gt; &lt;h2&gt;The glibc ABI impact&lt;/h2&gt; &lt;p&gt;Restartable sequences involve a memory area shared between userspace and kernel, which is updated by the kernel. The Linux kernel only supports one such area per thread, which means that all rseq-using code in the process needs to use that single area. Once glibc starts using rseq for its sched_getcpu function, direct activation of the rseq area via the rseq system call fails in applications because it has already been registered by glibc. Therefore, applications need to reuse the glibc-managed rseq area. In the glibc 2.35 upstream version, we solved this coordination problem by exposing &lt;a href="https://www.gnu.org/software/libc/manual/html_node/Restartable-Sequences.html"&gt;three new symbols&lt;/a&gt;: __rseq_size, __rseq_flags, __rseq_offset as part of the glibc ABI (Application Binary Interface). Applications that want to use the rseq facility can use these ABI symbols to locate the rseq area and use it according to the restartable sequences protocol.&lt;/p&gt; &lt;p&gt;RHEL 9 is based on glibc 2.34, not 2.35, so it does not immediately benefit from this upstream work. Initially, we backported the rseq-enhanced sched_getcpu into Red Hat Enterprise Linux 9.0 in a disabled-by-default configuration without the new symbols. Users could activate the glibc.pthread.rseq &lt;a href="https://www.gnu.org/software/libc/manual/html_node/Tunables.html"&gt;glibc tunable&lt;/a&gt; to enable it. This means that applications can register rseq directly with the kernel, but sched_getcpu on AArch64 remains very slow until the tunable is set manually on a per-process basis. In Red Hat Enterprise Linux 9.1, we will enable glibc's use of restartable sequences by default. But this means we also need to provide a coordination mechanism for applications to use. We could have come up with a RHEL-specific approach or a custom version of &lt;a href="https://github.com/compudj/librseq"&gt;librseq&lt;/a&gt; that applications are expected to use on RHEL. But in the end, we decided to backport the rseq-specific parts of the glibc 2.35 ABI into the glibc version that will be released with Red Hat Enterprise Linux 9.1. We have &lt;a href="https://developers.redhat.com/blog/2016/02/17/upgrading-the-gnu-c-library-within-red-hat-enterprise-linux"&gt;traditionally not augmented the glibc ABI after a release&lt;/a&gt;, but we felt that the use case was important enough to make an exception here.&lt;/p&gt; &lt;p&gt;Our RPM-based dependency management does not work on individual symbols (unlike &lt;a href="https://wiki.debian.org/UsingSymbolsFiles"&gt;Debian's symbol files&lt;/a&gt;). This means that, in general, we have to backport all symbols in the version set rather than the few we are actually interested in. In this case, we knew about the possible backport requirement when making the change upstream, so we made sure that the relevant symbol set included just the three symbols: __rseq_size, __rseq_flags, and __rseq_offset.&lt;/p&gt; &lt;p&gt;To maintain a consistent glibc ABI across the entire RHEL 9 release, a &lt;a href="https://access.redhat.com/errata/RHBA-2022:6605"&gt;RHEL 9.0 glibc update&lt;/a&gt; includes these new symbols as well. However, unlike RHEL 9.1, glibc's own use of rseq remains disabled by default. This additional backport ensures that rseq-enabled applications built against RHEL 9.1 or later can still launch on RHEL 9.0, albeit by default, without the glibc-integrated rseq facility.&lt;/p&gt; &lt;p&gt;The glibc.pthread.rseq tunable remains available in future RHEL 9 glibc versions and can be used to disable glibc's use of rseq in case there are compatibility issues. For example, we &lt;a href="https://github.com/checkpoint-restore/criu/pull/1706"&gt;worked with the CRIU authors&lt;/a&gt; to make sure that checkpointing and restoring applications continue despite rseq usage by the process, but for compatibility with checkpointing tools that lack rseq awareness, glibc's use of the feature can be disabled. Likewise, disabling glibc's use makes rseq registration available once more to early adopters of rseq that have not been ported yet to the glibc coordination mechanism (&lt;a href="https://github.com/google/tcmalloc/issues/1445%20github"&gt;such as tcmalloc&lt;/a&gt;).&lt;/p&gt; &lt;h2&gt;Restartable sequences availability&lt;/h2&gt; &lt;p&gt;Starting with Red Hat Enterprise Linux 9.1, restartable sequences are available as part of the system C library, and they are used to accelerate the sched_getcpu function that is critical to some database workloads. Maintaining compatibility with future rseq-using applications required adding new ABI symbols. A glibc update has been released for RHEL 9.0 to maintain a consistent glibc across all RHEL 9 minor releases.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/12/22/restartable-sequences-support-glibc-rhel-9" title="Why we added restartable sequences support to glibc in RHEL 9"&gt;Why we added restartable sequences support to glibc in RHEL 9&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Florian Weimer</dc:creator><dc:date>2022-12-22T07:00:00Z</dc:date></entry><entry><title type="html">AMA Recap</title><link rel="alternate" href="https://wildfly-security.github.io/wildfly-elytron/blog/ask-me-anything-recap/" /><author><name>Farah Juma</name></author><id>https://wildfly-security.github.io/wildfly-elytron/blog/ask-me-anything-recap/</id><updated>2022-12-22T00:00:00Z</updated><dc:creator>Farah Juma</dc:creator></entry></feed>
